{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "52fbb640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "\n",
    "# API 信息\n",
    "search_url = \"https://www.googleapis.com/youtube/v3/search\"\n",
    "API_keys = \"\" # 用自己的\n",
    "\n",
    "# 频道信息\n",
    "channel_list = {\"BBC\":\"UC16niRr50-MSBwiO3YDb3RA\",\n",
    "                \"Guardian\":\"UCIRYBXDze5krPDzAEOxFGVA\",\n",
    "                \"Sun\":\"UCIzXayRP7-P0ANpq-nD-h5g\",\n",
    "                \"DailyMail\":\"UCw3fku0sH3qA3c3pZeJwdAw\",\n",
    "                \"Independent\":\"UCshwRhftzkiov5wKR7M_LsQ\"}\n",
    "\n",
    "def getHTMLText(url, kv):\n",
    "    try:\n",
    "        # r = requests.get(url, timeout=30)\n",
    "        r = requests.request('GET', search_url, params = kv, timeout=30)\n",
    "        # print(r.url)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        print(\"爬取失败\")\n",
    "        return \"Error\"\n",
    "\n",
    "def getVideosbyWeek(channel_name, year, first_week, last_week):\n",
    "    # 设置输出地址\n",
    "    output_dir = \"/videolist/%s/json_byWeek/\"%(channel_name)\n",
    "    folder = os.path.exists(output_dir)\n",
    "    if not folder:\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    ChannelId = channel_list[channel_name]\n",
    "    # 按照周数，转换成日期，每次抓取一周内的新闻 \n",
    "    start_week = \"%s-W%s\"%(str(year),str(first_week))\n",
    "    start_day = datetime.datetime.strptime(start_week + '-1', \"%Y-W%W-%w\")\n",
    "    publishedAfter = str(start_day)[:10]+\"T00:00:00Z\"\n",
    "    \n",
    "    for i in range(first_week + 1 , last_week):\n",
    "        week_i = \"%s-W%s\"%(str(year),str(i))\n",
    "        start_i = datetime.datetime.strptime(week_i + '-1', \"%Y-W%W-%w\")\n",
    "        week_i_Mon = str(start_i)[:10]\n",
    "        # print(str(r)[:10])\n",
    "        publishedBefore= week_i_Mon+\"T00:00:00Z\"\n",
    "        print(publishedAfter, publishedBefore)\n",
    "\n",
    "        param_kv = {'key':API_keys, \\\n",
    "                    'channelId':ChannelId, \\\n",
    "                    'part': \"snippet,id\", \\\n",
    "                    'order':'date', \\\n",
    "                    'publishedAfter':publishedAfter, \\\n",
    "                    'publishedBefore':publishedBefore, \\\n",
    "                    'maxResults':50, \\\n",
    "                    'safeSearch':\"none\"}\n",
    "\n",
    "        result = getHTMLText(search_url, param_kv)\n",
    "        data_dict = json.loads(result)\n",
    "        with open(output_dir + 'result_%s_week_%s.json'%(str(year),str(i)), 'w') as f:\n",
    "            json.dump(data_dict, f)\n",
    "        publishedAfter = publishedBefore\n",
    "\n",
    "def getVideosbyDay(channel_name, first_day, last_day):\n",
    "    \"\"\"\n",
    "    输入格式：2021-09-30\n",
    "    \"\"\"\n",
    "    # 设置输出地址\n",
    "    output_dir_json = \"./videolist/%s/json_byDay/\"%(channel_name)\n",
    "    output_dir_csv = \"./videolist/%s/csv_byDay/\"%(channel_name)\n",
    "    #folder1 = os.path.exists(output_dir_json)\n",
    "    #folder2 = os.path.exists(output_dir_csv)\n",
    "    if not os.path.exists(output_dir_json):\n",
    "        os.makedirs(output_dir_json)\n",
    "    if not os.path.exists(output_dir_csv):\n",
    "        os.makedirs(output_dir_csv)\n",
    "        \n",
    "    ChannelId = channel_list[channel_name]\n",
    "    # 调整日期格式\n",
    "    d1 = datetime.strptime(first_day, '%Y-%m-%d')\n",
    "    d2 = datetime.strptime(last_day, '%Y-%m-%d')\n",
    "    \n",
    "    d_now = d1\n",
    "    d_now_format = str(d_now)[:10]+\"T00:00:00Z\"\n",
    "    publishedAfter = d_now_format\n",
    "\n",
    "    while (d_now < d2):\n",
    "        d_now = d_now + timedelta(days=1)\n",
    "        # 日期格式：1970-01-01T00:00:00Z\n",
    "        d_now_format = str(d_now)[:10]+\"T00:00:00Z\"\n",
    "        publishedBefore = d_now_format\n",
    "        print(\"Processing: \",publishedAfter, publishedBefore)\n",
    "        \n",
    "        param_kv = {'key':API_keys, \\\n",
    "                    'channelId':ChannelId, \\\n",
    "                    'part': \"snippet,id\", \\\n",
    "                    'order':'date', \\\n",
    "                    'publishedAfter':publishedAfter, \\\n",
    "                    'publishedBefore':publishedBefore, \\\n",
    "                    'maxResults':50, \\\n",
    "                    'safeSearch':\"none\"}\n",
    "\n",
    "        result = getHTMLText(search_url, param_kv)\n",
    "        data_dict = json.loads(result)\n",
    "        \n",
    "        # 把结果输出到\n",
    "        with open(output_dir_json + 'result_%s.json'%(publishedAfter[:10]), 'w') as f:\n",
    "            json.dump(data_dict, f)\n",
    "        # 处理为csv\n",
    "        data_df = jsonToDf(data_dict)\n",
    "        data_df.to_csv(output_dir_csv + 'result_%s.csv'%(publishedAfter[:10]),header = True)\n",
    "\n",
    "        publishedAfter = publishedBefore\n",
    "\n",
    "def jsonToDf(data_json):\n",
    "    \"\"\"\n",
    "    读取json数据，处理成DataFrame\n",
    "    \"\"\"\n",
    "    # 原始数据\n",
    "    df_data = pd.DataFrame(data_json['items'], columns=['id','snippet'])\n",
    "    # 初始化一个空df\n",
    "    column_names = ['channelId', \n",
    "                    'channelTitle',\n",
    "                    'videoId',\n",
    "                    'vidoeTitle',\n",
    "                    'description',\n",
    "                    'link','time',\n",
    "                    'year',\n",
    "                    'month',\n",
    "                    'day']\n",
    "    df = pd.DataFrame(columns = column_names)\n",
    "\n",
    "    for i in range(len(df_data)):\n",
    "        # json里大部分是video有一些是playlist，会报错\n",
    "        if (df_data['id'][i]['kind'] == \"youtube#playlist\"):\n",
    "            continue\n",
    "        # 最终的df会和原本的df_data长度不一样，所以用append来搞\n",
    "        datetime = df_data['snippet'][i]['publishedAt']\n",
    "        new_row = pd.DataFrame({\"videoId\" : df_data['id'][i]['videoId'],\n",
    "                                \"vidoeTitle\" : df_data['snippet'][i]['title'],\n",
    "                                \"channelId\" : df_data['snippet'][i]['channelId'],\n",
    "                                \"channelTitle\" : df_data['snippet'][i]['channelTitle'],\n",
    "                                \"description\" : df_data['snippet'][i]['description'],\n",
    "                                \"link\" : \"https://www.youtube.com/watch?v=\"+df_data['id'][i]['videoId'],\n",
    "                                \"time\" : datetime,\n",
    "                                \"year\" : int(datetime[:4]),\n",
    "                                \"month\" : int(datetime[5:7]),\n",
    "                                \"day\" : int(datetime[8:10]),\n",
    "                               },\n",
    "                               index=[\"0\"]) \n",
    "        df = df.append(new_row,ignore_index=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def combineCsv(channel_name):\n",
    "    path = os.getcwd() + \"/videolist/%s/csv_byDay/\"%(channel_name)\n",
    "    csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "    column_names = ['channelId', \n",
    "                    'channelTitle',\n",
    "                    'videoId',\n",
    "                    'vidoeTitle',\n",
    "                    'description',\n",
    "                    'link','time',\n",
    "                    'year',\n",
    "                    'month',\n",
    "                    'day']\n",
    "    df_all = pd.DataFrame(columns = column_names)\n",
    "\n",
    "    for f in csv_files:\n",
    "        df = pd.read_csv(f)\n",
    "        df_all = df_all.append(df, ignore_index=True)\n",
    "    df_all = df_all.sort_values(by=['year','month','day'])\n",
    "    df_all = df_all.reset_index(drop=True)\n",
    "    df_all.to_csv('./videolist/%s/%s_videolist.csv'%(channel_name,channel_name),header = True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7361f554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  2020-03-01T00:00:00Z 2020-03-02T00:00:00Z\n",
      "Processing:  2020-03-02T00:00:00Z 2020-03-03T00:00:00Z\n",
      "Processing:  2020-03-03T00:00:00Z 2020-03-04T00:00:00Z\n",
      "Processing:  2020-03-04T00:00:00Z 2020-03-05T00:00:00Z\n",
      "Processing:  2020-03-05T00:00:00Z 2020-03-06T00:00:00Z\n",
      "Processing:  2020-03-06T00:00:00Z 2020-03-07T00:00:00Z\n",
      "Processing:  2020-03-07T00:00:00Z 2020-03-08T00:00:00Z\n",
      "Processing:  2020-03-08T00:00:00Z 2020-03-09T00:00:00Z\n",
      "Processing:  2020-03-09T00:00:00Z 2020-03-10T00:00:00Z\n",
      "Processing:  2020-03-10T00:00:00Z 2020-03-11T00:00:00Z\n",
      "Processing:  2020-03-11T00:00:00Z 2020-03-12T00:00:00Z\n",
      "Processing:  2020-03-12T00:00:00Z 2020-03-13T00:00:00Z\n",
      "Processing:  2020-03-13T00:00:00Z 2020-03-14T00:00:00Z\n",
      "Processing:  2020-03-14T00:00:00Z 2020-03-15T00:00:00Z\n",
      "Processing:  2020-03-15T00:00:00Z 2020-03-16T00:00:00Z\n",
      "Processing:  2020-03-16T00:00:00Z 2020-03-17T00:00:00Z\n",
      "Processing:  2020-03-17T00:00:00Z 2020-03-18T00:00:00Z\n",
      "Processing:  2020-03-18T00:00:00Z 2020-03-19T00:00:00Z\n",
      "Processing:  2020-03-19T00:00:00Z 2020-03-20T00:00:00Z\n",
      "Processing:  2020-03-20T00:00:00Z 2020-03-21T00:00:00Z\n",
      "Processing:  2020-03-21T00:00:00Z 2020-03-22T00:00:00Z\n",
      "Processing:  2020-03-22T00:00:00Z 2020-03-23T00:00:00Z\n",
      "Processing:  2020-03-23T00:00:00Z 2020-03-24T00:00:00Z\n",
      "Processing:  2020-03-24T00:00:00Z 2020-03-25T00:00:00Z\n",
      "Processing:  2020-03-25T00:00:00Z 2020-03-26T00:00:00Z\n",
      "Processing:  2020-03-26T00:00:00Z 2020-03-27T00:00:00Z\n",
      "Processing:  2020-03-27T00:00:00Z 2020-03-28T00:00:00Z\n",
      "Processing:  2020-03-28T00:00:00Z 2020-03-29T00:00:00Z\n",
      "Processing:  2020-03-29T00:00:00Z 2020-03-30T00:00:00Z\n",
      "Processing:  2020-03-30T00:00:00Z 2020-03-31T00:00:00Z\n",
      "Processing:  2020-03-31T00:00:00Z 2020-04-01T00:00:00Z\n",
      "Processing:  2020-04-01T00:00:00Z 2020-04-02T00:00:00Z\n",
      "Processing:  2020-04-02T00:00:00Z 2020-04-03T00:00:00Z\n",
      "Processing:  2020-04-03T00:00:00Z 2020-04-04T00:00:00Z\n",
      "Processing:  2020-04-04T00:00:00Z 2020-04-05T00:00:00Z\n",
      "Processing:  2020-04-05T00:00:00Z 2020-04-06T00:00:00Z\n",
      "Processing:  2020-04-06T00:00:00Z 2020-04-07T00:00:00Z\n",
      "Processing:  2020-04-07T00:00:00Z 2020-04-08T00:00:00Z\n",
      "Processing:  2020-04-08T00:00:00Z 2020-04-09T00:00:00Z\n",
      "Processing:  2020-04-09T00:00:00Z 2020-04-10T00:00:00Z\n",
      "Processing:  2020-04-10T00:00:00Z 2020-04-11T00:00:00Z\n",
      "Processing:  2020-04-11T00:00:00Z 2020-04-12T00:00:00Z\n",
      "Processing:  2020-04-12T00:00:00Z 2020-04-13T00:00:00Z\n",
      "Processing:  2020-04-13T00:00:00Z 2020-04-14T00:00:00Z\n",
      "Processing:  2020-04-14T00:00:00Z 2020-04-15T00:00:00Z\n",
      "Processing:  2020-04-15T00:00:00Z 2020-04-16T00:00:00Z\n",
      "Processing:  2020-04-16T00:00:00Z 2020-04-17T00:00:00Z\n",
      "Processing:  2020-04-17T00:00:00Z 2020-04-18T00:00:00Z\n",
      "Processing:  2020-04-18T00:00:00Z 2020-04-19T00:00:00Z\n",
      "Processing:  2020-04-19T00:00:00Z 2020-04-20T00:00:00Z\n",
      "Processing:  2020-04-20T00:00:00Z 2020-04-21T00:00:00Z\n",
      "Processing:  2020-04-21T00:00:00Z 2020-04-22T00:00:00Z\n",
      "Processing:  2020-04-22T00:00:00Z 2020-04-23T00:00:00Z\n",
      "Processing:  2020-04-23T00:00:00Z 2020-04-24T00:00:00Z\n",
      "Processing:  2020-04-24T00:00:00Z 2020-04-25T00:00:00Z\n",
      "Processing:  2020-04-25T00:00:00Z 2020-04-26T00:00:00Z\n",
      "Processing:  2020-04-26T00:00:00Z 2020-04-27T00:00:00Z\n",
      "Processing:  2020-04-27T00:00:00Z 2020-04-28T00:00:00Z\n",
      "Processing:  2020-04-28T00:00:00Z 2020-04-29T00:00:00Z\n",
      "Processing:  2020-04-29T00:00:00Z 2020-04-30T00:00:00Z\n",
      "Processing:  2020-04-30T00:00:00Z 2020-05-01T00:00:00Z\n",
      "Processing:  2020-05-01T00:00:00Z 2020-05-02T00:00:00Z\n",
      "Processing:  2020-05-02T00:00:00Z 2020-05-03T00:00:00Z\n",
      "Processing:  2020-05-03T00:00:00Z 2020-05-04T00:00:00Z\n",
      "Processing:  2020-05-04T00:00:00Z 2020-05-05T00:00:00Z\n",
      "Processing:  2020-05-05T00:00:00Z 2020-05-06T00:00:00Z\n",
      "Processing:  2020-05-06T00:00:00Z 2020-05-07T00:00:00Z\n",
      "Processing:  2020-05-07T00:00:00Z 2020-05-08T00:00:00Z\n",
      "Processing:  2020-05-08T00:00:00Z 2020-05-09T00:00:00Z\n",
      "Processing:  2020-05-09T00:00:00Z 2020-05-10T00:00:00Z\n",
      "Processing:  2020-05-10T00:00:00Z 2020-05-11T00:00:00Z\n",
      "Processing:  2020-05-11T00:00:00Z 2020-05-12T00:00:00Z\n",
      "Processing:  2020-05-12T00:00:00Z 2020-05-13T00:00:00Z\n",
      "Processing:  2020-05-13T00:00:00Z 2020-05-14T00:00:00Z\n",
      "Processing:  2020-05-14T00:00:00Z 2020-05-15T00:00:00Z\n",
      "Processing:  2020-05-15T00:00:00Z 2020-05-16T00:00:00Z\n",
      "Processing:  2020-05-16T00:00:00Z 2020-05-17T00:00:00Z\n",
      "Processing:  2020-05-17T00:00:00Z 2020-05-18T00:00:00Z\n",
      "Processing:  2020-05-18T00:00:00Z 2020-05-19T00:00:00Z\n",
      "Processing:  2020-05-19T00:00:00Z 2020-05-20T00:00:00Z\n",
      "Processing:  2020-05-20T00:00:00Z 2020-05-21T00:00:00Z\n",
      "Processing:  2020-05-21T00:00:00Z 2020-05-22T00:00:00Z\n",
      "Processing:  2020-05-22T00:00:00Z 2020-05-23T00:00:00Z\n",
      "Processing:  2020-05-23T00:00:00Z 2020-05-24T00:00:00Z\n",
      "Processing:  2020-05-24T00:00:00Z 2020-05-25T00:00:00Z\n",
      "Processing:  2020-05-25T00:00:00Z 2020-05-26T00:00:00Z\n",
      "Processing:  2020-05-26T00:00:00Z 2020-05-27T00:00:00Z\n",
      "Processing:  2020-05-27T00:00:00Z 2020-05-28T00:00:00Z\n",
      "Processing:  2020-05-28T00:00:00Z 2020-05-29T00:00:00Z\n",
      "Processing:  2020-05-29T00:00:00Z 2020-05-30T00:00:00Z\n",
      "Processing:  2020-05-30T00:00:00Z 2020-05-31T00:00:00Z\n",
      "Processing:  2020-05-31T00:00:00Z 2020-06-01T00:00:00Z\n",
      "Processing:  2020-06-01T00:00:00Z 2020-06-02T00:00:00Z\n",
      "Processing:  2020-06-02T00:00:00Z 2020-06-03T00:00:00Z\n",
      "Processing:  2020-06-03T00:00:00Z 2020-06-04T00:00:00Z\n",
      "Processing:  2020-06-04T00:00:00Z 2020-06-05T00:00:00Z\n",
      "Processing:  2020-06-05T00:00:00Z 2020-06-06T00:00:00Z\n",
      "Processing:  2020-06-06T00:00:00Z 2020-06-07T00:00:00Z\n",
      "Processing:  2020-06-07T00:00:00Z 2020-06-08T00:00:00Z\n",
      "Processing:  2020-06-08T00:00:00Z 2020-06-09T00:00:00Z\n",
      "Processing:  2020-06-09T00:00:00Z 2020-06-10T00:00:00Z\n",
      "爬取失败\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/17/y0w1c5w93gn2rq598t4st47r0000gn/T/ipykernel_69894/1043601731.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgetVideosbyDay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sun\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"2020-03-01\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"2021-05-01\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# API配额用完了会报错\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/17/y0w1c5w93gn2rq598t4st47r0000gn/T/ipykernel_69894/632851673.py\u001b[0m in \u001b[0;36mgetVideosbyDay\u001b[0;34m(channel_name, first_day, last_day)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetHTMLText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_kv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mdata_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m# 把结果输出到\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/socialEnv/lib/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/socialEnv/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/socialEnv/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "getVideosbyDay(\"Guardian\",\"2020-03-01\",\"2021-05-01\")\n",
    "# 每天只能进行100次search，API配额用完了会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8e2a5629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并csv文件到大表\n",
    "combineCsv(\"Guradian\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:socialEnv] *",
   "language": "python",
   "name": "conda-env-socialEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
