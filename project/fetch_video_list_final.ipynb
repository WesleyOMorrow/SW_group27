{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "\n",
    "# API \n",
    "search_url = \"https://www.googleapis.com/youtube/v3/search\"\n",
    "API_keys = \"\" # Use your own API\n",
    "\n",
    "# The channel list\n",
    "channel_list = {\"BBC\":\"UC16niRr50-MSBwiO3YDb3RA\",\n",
    "                \"Guardian\":\"UCIRYBXDze5krPDzAEOxFGVA\",\n",
    "                \"Sun\":\"UCIzXayRP7-P0ANpq-nD-h5g\",\n",
    "                \"DailyMail\":\"UCw3fku0sH3qA3c3pZeJwdAw\",\n",
    "                \"Independent\":\"UCshwRhftzkiov5wKR7M_LsQ\"}\n",
    "\n",
    "def getHTMLText(url, kv):\n",
    "    try:\n",
    "        # r = requests.get(url, timeout=30)\n",
    "        r = requests.request('GET', search_url, params = kv, timeout=30)\n",
    "        # print(r.url)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        print(\"Out of quota!\")\n",
    "        return \"Error\"\n",
    "\n",
    "def getVideosbyWeek(channel_name, year, first_week, last_week):\n",
    "    # Output dir\n",
    "    output_dir_json = \"./videolist/%s/json_byWeek/\"%(channel_name)\n",
    "    output_dir_csv = \"./videolist/%s/csv_byWeek/\"%(channel_name)\n",
    "    #folder1 = os.path.exists(output_dir_json)\n",
    "    #folder2 = os.path.exists(output_dir_csv)\n",
    "    if not os.path.exists(output_dir_json):\n",
    "        os.makedirs(output_dir_json)\n",
    "    if not os.path.exists(output_dir_csv):\n",
    "        os.makedirs(output_dir_csv)        \n",
    "    \n",
    "    ChannelId = channel_list[channel_name]\n",
    "    # Convert number of week to date\n",
    "    # Capture news within one week at one time\n",
    "    start_week = \"%s-W%s\"%(str(year),str(first_week))\n",
    "    start_day = datetime.strptime(start_week + '-1', \"%Y-W%W-%w\")\n",
    "    publishedAfter = str(start_day)[:10]+\"T00:00:00Z\"\n",
    "    \n",
    "    for i in range(first_week + 1 , last_week):\n",
    "        week_i = \"%s-W%s\"%(str(year),str(i))\n",
    "        start_i = datetime.strptime(week_i + '-1', \"%Y-W%W-%w\")\n",
    "        week_i_Mon = str(start_i)[:10]\n",
    "        # print(str(r)[:10])\n",
    "        publishedBefore= week_i_Mon+\"T00:00:00Z\"\n",
    "        print(publishedAfter, publishedBefore)\n",
    "\n",
    "        param_kv = {'key':API_keys, \\\n",
    "                    'channelId':ChannelId, \\\n",
    "                    'part': \"snippet,id\", \\\n",
    "                    'order':'date', \\\n",
    "                    'publishedAfter':publishedAfter, \\\n",
    "                    'publishedBefore':publishedBefore, \\\n",
    "                    'maxResults':50, \\\n",
    "                    'safeSearch':\"none\"}\n",
    "\n",
    "        result = getHTMLText(search_url, param_kv)\n",
    "        data_dict = json.loads(result)\n",
    "        with open(output_dir_json + 'result_%s_week_%s.json'%(str(year),str(i)), 'w') as f:\n",
    "            json.dump(data_dict, f)\n",
    "        # output to csv\n",
    "        data_df = jsonToDf(data_dict)\n",
    "        data_df.to_csv(output_dir_csv + 'result_%s_week_%s.csv'%(str(year),str(i)),header = True)\n",
    "        publishedAfter = publishedBefore\n",
    "\n",
    "def getVideosbyDay(channel_name, first_day, last_day):\n",
    "    \"\"\"\n",
    "    input date format：2021-09-30\n",
    "    \"\"\"\n",
    "    # set output dir\n",
    "    output_dir_json = \"./videolist/%s/json_byDay/\"%(channel_name)\n",
    "    output_dir_csv = \"./videolist/%s/csv_byDay/\"%(channel_name)\n",
    "    #folder1 = os.path.exists(output_dir_json)\n",
    "    #folder2 = os.path.exists(output_dir_csv)\n",
    "    if not os.path.exists(output_dir_json):\n",
    "        os.makedirs(output_dir_json)\n",
    "    if not os.path.exists(output_dir_csv):\n",
    "        os.makedirs(output_dir_csv)\n",
    "        \n",
    "    ChannelId = channel_list[channel_name]\n",
    "    # adjust date format\n",
    "    d1 = datetime.strptime(first_day, '%Y-%m-%d')\n",
    "    d2 = datetime.strptime(last_day, '%Y-%m-%d')\n",
    "    \n",
    "    d_now = d1\n",
    "    d_now_format = str(d_now)[:10]+\"T00:00:00Z\"\n",
    "    publishedAfter = d_now_format\n",
    "\n",
    "    while (d_now < d2):\n",
    "        d_now = d_now + timedelta(days=1)\n",
    "        # datetime format：1970-01-01T00:00:00Z\n",
    "        d_now_format = str(d_now)[:10]+\"T00:00:00Z\"\n",
    "        publishedBefore = d_now_format\n",
    "        print(\"Processing: \",publishedAfter, publishedBefore)\n",
    "        \n",
    "        param_kv = {'key':API_keys, \\\n",
    "                    'channelId':ChannelId, \\\n",
    "                    'part': \"snippet,id\", \\\n",
    "                    'order':'date', \\\n",
    "                    'publishedAfter':publishedAfter, \\\n",
    "                    'publishedBefore':publishedBefore, \\\n",
    "                    'maxResults':50, \\\n",
    "                    'safeSearch':\"none\"}\n",
    "\n",
    "        result = getHTMLText(search_url, param_kv)\n",
    "        data_dict = json.loads(result)\n",
    "        \n",
    "        # output to json\n",
    "        with open(output_dir_json + 'result_%s.json'%(publishedAfter[:10]), 'w') as f:\n",
    "            json.dump(data_dict, f)\n",
    "        # output to csv\n",
    "        data_df = jsonToDf(data_dict)\n",
    "        data_df.to_csv(output_dir_csv + 'result_%s.csv'%(publishedAfter[:10]),header = True)\n",
    "\n",
    "        publishedAfter = publishedBefore\n",
    "\n",
    "def jsonToDf(data_json):\n",
    "    \"\"\"\n",
    "    take json，output to DataFrame\n",
    "    \"\"\"\n",
    "    # original data\n",
    "    df_data = pd.DataFrame(data_json['items'], columns=['id','snippet'])\n",
    "    # initial empty df\n",
    "    column_names = ['channelId', \n",
    "                    'channelTitle',\n",
    "                    'videoId',\n",
    "                    'vidoeTitle',\n",
    "                    'description',\n",
    "                    'link','time',\n",
    "                    'year',\n",
    "                    'month',\n",
    "                    'day']\n",
    "    df = pd.DataFrame(columns = column_names)\n",
    "\n",
    "    for i in range(len(df_data)):\n",
    "        # filter out the playlist in the json results\n",
    "        if (df_data['id'][i]['kind'] == \"youtube#playlist\"):\n",
    "            continue\n",
    "        # use append to avoid different lengths\n",
    "        datetime = df_data['snippet'][i]['publishedAt']\n",
    "        new_row = pd.DataFrame({\"videoId\" : df_data['id'][i]['videoId'],\n",
    "                                \"vidoeTitle\" : df_data['snippet'][i]['title'],\n",
    "                                \"channelId\" : df_data['snippet'][i]['channelId'],\n",
    "                                \"channelTitle\" : df_data['snippet'][i]['channelTitle'],\n",
    "                                \"description\" : df_data['snippet'][i]['description'],\n",
    "                                \"link\" : \"https://www.youtube.com/watch?v=\"+df_data['id'][i]['videoId'],\n",
    "                                \"time\" : datetime,\n",
    "                                \"year\" : int(datetime[:4]),\n",
    "                                \"month\" : int(datetime[5:7]),\n",
    "                                \"day\" : int(datetime[8:10]),\n",
    "                               },\n",
    "                               index=[\"0\"]) \n",
    "        df = df.append(new_row,ignore_index=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def combineCsv(channel_name, byWhat):\n",
    "    \"\"\"\n",
    "    byWhat = \"Week\" or \"Day\"\n",
    "    \"\"\"\n",
    "    path = os.getcwd() + \"/videolist/%s/csv_by%s/\"%(channel_name,byWhat)\n",
    "    csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "    column_names = ['channelId', \n",
    "                    'channelTitle',\n",
    "                    'videoId',\n",
    "                    'vidoeTitle',\n",
    "                    'description',\n",
    "                    'link','time',\n",
    "                    'year',\n",
    "                    'month',\n",
    "                    'day']\n",
    "    df_all = pd.DataFrame(columns = column_names)\n",
    "\n",
    "    for f in csv_files:\n",
    "        df = pd.read_csv(f)\n",
    "        df_all = df_all.append(df, ignore_index=True)\n",
    "        # Find overflow weeks(contains >50 news a week)\n",
    "        if (len(df) >= 50):\n",
    "            print(f.split('/')[-1])\n",
    "            # print date of overflow week\n",
    "            miss_last = df.loc[0,'time'][:10]\n",
    "            miss_first = datetime.strptime(miss_last, '%Y-%m-%d') - timedelta(days=7)\n",
    "            miss_first = str(miss_first)[:10]\n",
    "            print(miss_first, miss_last)\n",
    "    df_all = df_all.sort_values(by=['year','month','day'])\n",
    "    df_all = df_all.drop_duplicates(keep='first')\n",
    "    df_all = df_all.reset_index(drop=True)\n",
    "    df_all = df_all.drop(['Unnamed: 0'],axis=1)\n",
    "    df_all.to_csv('./videolist/%s/%s_videolist_by%s.csv'%(channel_name,channel_name,byWhat),header = True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-30T00:00:00Z 2020-01-06T00:00:00Z\n",
      "2020-01-06T00:00:00Z 2020-01-13T00:00:00Z\n",
      "2020-01-13T00:00:00Z 2020-01-20T00:00:00Z\n",
      "2020-01-20T00:00:00Z 2020-01-27T00:00:00Z\n",
      "2020-01-27T00:00:00Z 2020-02-03T00:00:00Z\n",
      "2020-02-03T00:00:00Z 2020-02-10T00:00:00Z\n",
      "2020-02-10T00:00:00Z 2020-02-17T00:00:00Z\n",
      "2020-02-17T00:00:00Z 2020-02-24T00:00:00Z\n",
      "2020-02-24T00:00:00Z 2020-03-02T00:00:00Z\n",
      "2020-03-02T00:00:00Z 2020-03-09T00:00:00Z\n",
      "2020-03-09T00:00:00Z 2020-03-16T00:00:00Z\n",
      "2020-03-16T00:00:00Z 2020-03-23T00:00:00Z\n",
      "2020-03-23T00:00:00Z 2020-03-30T00:00:00Z\n",
      "2020-03-30T00:00:00Z 2020-04-06T00:00:00Z\n",
      "2020-04-06T00:00:00Z 2020-04-13T00:00:00Z\n",
      "2020-04-13T00:00:00Z 2020-04-20T00:00:00Z\n",
      "2020-04-20T00:00:00Z 2020-04-27T00:00:00Z\n",
      "2020-04-27T00:00:00Z 2020-05-04T00:00:00Z\n",
      "2020-05-04T00:00:00Z 2020-05-11T00:00:00Z\n",
      "2020-05-11T00:00:00Z 2020-05-18T00:00:00Z\n",
      "2020-05-18T00:00:00Z 2020-05-25T00:00:00Z\n",
      "2020-05-25T00:00:00Z 2020-06-01T00:00:00Z\n",
      "2020-06-01T00:00:00Z 2020-06-08T00:00:00Z\n",
      "2020-06-08T00:00:00Z 2020-06-15T00:00:00Z\n",
      "2020-06-15T00:00:00Z 2020-06-22T00:00:00Z\n",
      "2020-06-22T00:00:00Z 2020-06-29T00:00:00Z\n",
      "2020-06-29T00:00:00Z 2020-07-06T00:00:00Z\n",
      "2020-07-06T00:00:00Z 2020-07-13T00:00:00Z\n",
      "2020-07-13T00:00:00Z 2020-07-20T00:00:00Z\n",
      "2020-07-20T00:00:00Z 2020-07-27T00:00:00Z\n",
      "2020-07-27T00:00:00Z 2020-08-03T00:00:00Z\n",
      "2020-08-03T00:00:00Z 2020-08-10T00:00:00Z\n",
      "2020-08-10T00:00:00Z 2020-08-17T00:00:00Z\n",
      "2020-08-17T00:00:00Z 2020-08-24T00:00:00Z\n",
      "2020-08-24T00:00:00Z 2020-08-31T00:00:00Z\n",
      "2020-08-31T00:00:00Z 2020-09-07T00:00:00Z\n",
      "2020-09-07T00:00:00Z 2020-09-14T00:00:00Z\n",
      "2020-09-14T00:00:00Z 2020-09-21T00:00:00Z\n",
      "2020-09-21T00:00:00Z 2020-09-28T00:00:00Z\n"
     ]
    }
   ],
   "source": [
    "# Download videolist by week(up to 50 news a week)\n",
    "getVideosbyWeek(\"Sun\",2020,0,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download videolist by day(up to 50 news a day)\n",
    "getVideosbyDay(\"DailyMail\",\"2020-09-29\",\"2021-07-01\")\n",
    "# quota: 100 search per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the results\n",
    "combineCsv(\"DailyMail\",\"Day\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:socialEnv]",
   "language": "python",
   "name": "conda-env-socialEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
