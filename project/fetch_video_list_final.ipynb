{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "52fbb640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "\n",
    "# API 信息\n",
    "search_url = \"https://www.googleapis.com/youtube/v3/search\"\n",
    "API_keys = \"\" # 用自己的\n",
    "\n",
    "# 频道信息\n",
    "channel_list = {\"BBC\":\"UC16niRr50-MSBwiO3YDb3RA\",\n",
    "                \"Guardian\":\"UCIRYBXDze5krPDzAEOxFGVA\",\n",
    "                \"Sun\":\"UCIzXayRP7-P0ANpq-nD-h5g\",\n",
    "                \"DailyMail\":\"UCw3fku0sH3qA3c3pZeJwdAw\",\n",
    "                \"Independent\":\"UCshwRhftzkiov5wKR7M_LsQ\"}\n",
    "\n",
    "def getHTMLText(url, kv):\n",
    "    try:\n",
    "        # r = requests.get(url, timeout=30)\n",
    "        r = requests.request('GET', search_url, params = kv, timeout=30)\n",
    "        # print(r.url)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        print(\"爬取失败\")\n",
    "        return \"Error\"\n",
    "\n",
    "def getVideosbyWeek(channel_name, year, first_week, last_week):\n",
    "    # 设置输出地址\n",
    "    output_dir_json = \"./videolist/%s/json_byWeek/\"%(channel_name)\n",
    "    output_dir_csv = \"./videolist/%s/csv_byWeek/\"%(channel_name)\n",
    "    #folder1 = os.path.exists(output_dir_json)\n",
    "    #folder2 = os.path.exists(output_dir_csv)\n",
    "    if not os.path.exists(output_dir_json):\n",
    "        os.makedirs(output_dir_json)\n",
    "    if not os.path.exists(output_dir_csv):\n",
    "        os.makedirs(output_dir_csv)        \n",
    "    \n",
    "    ChannelId = channel_list[channel_name]\n",
    "    # 按照周数，转换成日期，每次抓取一周内的新闻 \n",
    "    start_week = \"%s-W%s\"%(str(year),str(first_week))\n",
    "    start_day = datetime.strptime(start_week + '-1', \"%Y-W%W-%w\")\n",
    "    publishedAfter = str(start_day)[:10]+\"T00:00:00Z\"\n",
    "    \n",
    "    for i in range(first_week + 1 , last_week):\n",
    "        week_i = \"%s-W%s\"%(str(year),str(i))\n",
    "        start_i = datetime.strptime(week_i + '-1', \"%Y-W%W-%w\")\n",
    "        week_i_Mon = str(start_i)[:10]\n",
    "        # print(str(r)[:10])\n",
    "        publishedBefore= week_i_Mon+\"T00:00:00Z\"\n",
    "        print(publishedAfter, publishedBefore)\n",
    "\n",
    "        param_kv = {'key':API_keys, \\\n",
    "                    'channelId':ChannelId, \\\n",
    "                    'part': \"snippet,id\", \\\n",
    "                    'order':'date', \\\n",
    "                    'publishedAfter':publishedAfter, \\\n",
    "                    'publishedBefore':publishedBefore, \\\n",
    "                    'maxResults':50, \\\n",
    "                    'safeSearch':\"none\"}\n",
    "\n",
    "        result = getHTMLText(search_url, param_kv)\n",
    "        data_dict = json.loads(result)\n",
    "        with open(output_dir_json + 'result_%s_week_%s.json'%(str(year),str(i)), 'w') as f:\n",
    "            json.dump(data_dict, f)\n",
    "        # 处理为csv\n",
    "        data_df = jsonToDf(data_dict)\n",
    "        data_df.to_csv(output_dir_csv + 'result_%s_week_%s.csv'%(str(year),str(i)),header = True)\n",
    "        publishedAfter = publishedBefore\n",
    "\n",
    "def getVideosbyDay(channel_name, first_day, last_day):\n",
    "    \"\"\"\n",
    "    输入格式：2021-09-30\n",
    "    \"\"\"\n",
    "    # 设置输出地址\n",
    "    output_dir_json = \"./videolist/%s/json_byDay/\"%(channel_name)\n",
    "    output_dir_csv = \"./videolist/%s/csv_byDay/\"%(channel_name)\n",
    "    #folder1 = os.path.exists(output_dir_json)\n",
    "    #folder2 = os.path.exists(output_dir_csv)\n",
    "    if not os.path.exists(output_dir_json):\n",
    "        os.makedirs(output_dir_json)\n",
    "    if not os.path.exists(output_dir_csv):\n",
    "        os.makedirs(output_dir_csv)\n",
    "        \n",
    "    ChannelId = channel_list[channel_name]\n",
    "    # 调整日期格式\n",
    "    d1 = datetime.strptime(first_day, '%Y-%m-%d')\n",
    "    d2 = datetime.strptime(last_day, '%Y-%m-%d')\n",
    "    \n",
    "    d_now = d1\n",
    "    d_now_format = str(d_now)[:10]+\"T00:00:00Z\"\n",
    "    publishedAfter = d_now_format\n",
    "\n",
    "    while (d_now < d2):\n",
    "        d_now = d_now + timedelta(days=1)\n",
    "        # 日期格式：1970-01-01T00:00:00Z\n",
    "        d_now_format = str(d_now)[:10]+\"T00:00:00Z\"\n",
    "        publishedBefore = d_now_format\n",
    "        print(\"Processing: \",publishedAfter, publishedBefore)\n",
    "        \n",
    "        param_kv = {'key':API_keys, \\\n",
    "                    'channelId':ChannelId, \\\n",
    "                    'part': \"snippet,id\", \\\n",
    "                    'order':'date', \\\n",
    "                    'publishedAfter':publishedAfter, \\\n",
    "                    'publishedBefore':publishedBefore, \\\n",
    "                    'maxResults':50, \\\n",
    "                    'safeSearch':\"none\"}\n",
    "\n",
    "        result = getHTMLText(search_url, param_kv)\n",
    "        data_dict = json.loads(result)\n",
    "        \n",
    "        # 把结果输出到\n",
    "        with open(output_dir_json + 'result_%s.json'%(publishedAfter[:10]), 'w') as f:\n",
    "            json.dump(data_dict, f)\n",
    "        # 处理为csv\n",
    "        data_df = jsonToDf(data_dict)\n",
    "        data_df.to_csv(output_dir_csv + 'result_%s.csv'%(publishedAfter[:10]),header = True)\n",
    "\n",
    "        publishedAfter = publishedBefore\n",
    "\n",
    "def jsonToDf(data_json):\n",
    "    \"\"\"\n",
    "    读取json数据，处理成DataFrame\n",
    "    \"\"\"\n",
    "    # 原始数据\n",
    "    df_data = pd.DataFrame(data_json['items'], columns=['id','snippet'])\n",
    "    # 初始化一个空df\n",
    "    column_names = ['channelId', \n",
    "                    'channelTitle',\n",
    "                    'videoId',\n",
    "                    'vidoeTitle',\n",
    "                    'description',\n",
    "                    'link','time',\n",
    "                    'year',\n",
    "                    'month',\n",
    "                    'day']\n",
    "    df = pd.DataFrame(columns = column_names)\n",
    "\n",
    "    for i in range(len(df_data)):\n",
    "        # json里大部分是video有一些是playlist，把这部分滤掉\n",
    "        if (df_data['id'][i]['kind'] == \"youtube#playlist\"):\n",
    "            continue\n",
    "        # 最终的df会和原本的df_data长度不一样，所以用append来搞\n",
    "        datetime = df_data['snippet'][i]['publishedAt']\n",
    "        new_row = pd.DataFrame({\"videoId\" : df_data['id'][i]['videoId'],\n",
    "                                \"vidoeTitle\" : df_data['snippet'][i]['title'],\n",
    "                                \"channelId\" : df_data['snippet'][i]['channelId'],\n",
    "                                \"channelTitle\" : df_data['snippet'][i]['channelTitle'],\n",
    "                                \"description\" : df_data['snippet'][i]['description'],\n",
    "                                \"link\" : \"https://www.youtube.com/watch?v=\"+df_data['id'][i]['videoId'],\n",
    "                                \"time\" : datetime,\n",
    "                                \"year\" : int(datetime[:4]),\n",
    "                                \"month\" : int(datetime[5:7]),\n",
    "                                \"day\" : int(datetime[8:10]),\n",
    "                               },\n",
    "                               index=[\"0\"]) \n",
    "        df = df.append(new_row,ignore_index=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def combineCsv(channel_name, byWhat):\n",
    "    \"\"\"\n",
    "    byWhat = \"Week\" or \"Day\"\n",
    "    \"\"\"\n",
    "    path = os.getcwd() + \"/videolist/%s/csv_by%s/\"%(channel_name,byWhat)\n",
    "    csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "    column_names = ['channelId', \n",
    "                    'channelTitle',\n",
    "                    'videoId',\n",
    "                    'vidoeTitle',\n",
    "                    'description',\n",
    "                    'link','time',\n",
    "                    'year',\n",
    "                    'month',\n",
    "                    'day']\n",
    "    df_all = pd.DataFrame(columns = column_names)\n",
    "\n",
    "    for f in csv_files:\n",
    "        df = pd.read_csv(f)\n",
    "        df_all = df_all.append(df, ignore_index=True)\n",
    "        # 找出超出50条的周，然后输出\n",
    "        if (len(df) >= 50):\n",
    "            print(f.split('/')[-1])\n",
    "            # 打印这周的日期，方便单独爬\n",
    "            miss_last = df.loc[0,'time'][:10]\n",
    "            miss_first = datetime.strptime(miss_last, '%Y-%m-%d') - timedelta(days=7)\n",
    "            miss_first = str(miss_first)[:10]\n",
    "            print(miss_first, miss_last)\n",
    "    df_all = df_all.sort_values(by=['year','month','day'])\n",
    "    df_all = df_all.drop_duplicates(keep='first')\n",
    "    df_all = df_all.reset_index(drop=True)\n",
    "    df_all = df_all.drop(['Unnamed: 0'],axis=1)\n",
    "    df_all.to_csv('./videolist/%s/%s_videolist_by%s.csv'%(channel_name,channel_name,byWhat),header = True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e11ca979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-30T00:00:00Z 2020-01-06T00:00:00Z\n",
      "2020-01-06T00:00:00Z 2020-01-13T00:00:00Z\n",
      "2020-01-13T00:00:00Z 2020-01-20T00:00:00Z\n",
      "2020-01-20T00:00:00Z 2020-01-27T00:00:00Z\n",
      "2020-01-27T00:00:00Z 2020-02-03T00:00:00Z\n",
      "2020-02-03T00:00:00Z 2020-02-10T00:00:00Z\n",
      "2020-02-10T00:00:00Z 2020-02-17T00:00:00Z\n",
      "2020-02-17T00:00:00Z 2020-02-24T00:00:00Z\n",
      "2020-02-24T00:00:00Z 2020-03-02T00:00:00Z\n",
      "2020-03-02T00:00:00Z 2020-03-09T00:00:00Z\n",
      "2020-03-09T00:00:00Z 2020-03-16T00:00:00Z\n",
      "2020-03-16T00:00:00Z 2020-03-23T00:00:00Z\n",
      "2020-03-23T00:00:00Z 2020-03-30T00:00:00Z\n",
      "2020-03-30T00:00:00Z 2020-04-06T00:00:00Z\n",
      "2020-04-06T00:00:00Z 2020-04-13T00:00:00Z\n",
      "2020-04-13T00:00:00Z 2020-04-20T00:00:00Z\n",
      "2020-04-20T00:00:00Z 2020-04-27T00:00:00Z\n",
      "2020-04-27T00:00:00Z 2020-05-04T00:00:00Z\n",
      "2020-05-04T00:00:00Z 2020-05-11T00:00:00Z\n",
      "2020-05-11T00:00:00Z 2020-05-18T00:00:00Z\n",
      "2020-05-18T00:00:00Z 2020-05-25T00:00:00Z\n",
      "2020-05-25T00:00:00Z 2020-06-01T00:00:00Z\n",
      "2020-06-01T00:00:00Z 2020-06-08T00:00:00Z\n",
      "2020-06-08T00:00:00Z 2020-06-15T00:00:00Z\n",
      "2020-06-15T00:00:00Z 2020-06-22T00:00:00Z\n",
      "2020-06-22T00:00:00Z 2020-06-29T00:00:00Z\n",
      "2020-06-29T00:00:00Z 2020-07-06T00:00:00Z\n",
      "2020-07-06T00:00:00Z 2020-07-13T00:00:00Z\n",
      "2020-07-13T00:00:00Z 2020-07-20T00:00:00Z\n",
      "2020-07-20T00:00:00Z 2020-07-27T00:00:00Z\n",
      "2020-07-27T00:00:00Z 2020-08-03T00:00:00Z\n",
      "2020-08-03T00:00:00Z 2020-08-10T00:00:00Z\n",
      "2020-08-10T00:00:00Z 2020-08-17T00:00:00Z\n",
      "2020-08-17T00:00:00Z 2020-08-24T00:00:00Z\n",
      "2020-08-24T00:00:00Z 2020-08-31T00:00:00Z\n",
      "2020-08-31T00:00:00Z 2020-09-07T00:00:00Z\n",
      "2020-09-07T00:00:00Z 2020-09-14T00:00:00Z\n",
      "2020-09-14T00:00:00Z 2020-09-21T00:00:00Z\n",
      "2020-09-21T00:00:00Z 2020-09-28T00:00:00Z\n"
     ]
    }
   ],
   "source": [
    "# 按照周爬取（每周上限50条，有漏掉的可能，但是不浪费API）\n",
    "getVideosbyWeek(\"BBC\",2020,0,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7361f554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  2021-02-24T00:00:00Z 2021-02-25T00:00:00Z\n",
      "爬取失败\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/17/y0w1c5w93gn2rq598t4st47r0000gn/T/ipykernel_86927/534988413.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 按照日爬取（每日上限50条，不会漏，但是浪费API）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgetVideosbyDay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Guardian\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"2021-02-24\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"2021-04-30\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# 每天只能进行100次search，API配额用完了会报错\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/17/y0w1c5w93gn2rq598t4st47r0000gn/T/ipykernel_86927/3215944496.py\u001b[0m in \u001b[0;36mgetVideosbyDay\u001b[0;34m(channel_name, first_day, last_day)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetHTMLText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_kv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mdata_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# 把结果输出到\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/socialEnv/lib/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/socialEnv/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/socialEnv/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "# 按照日爬取（每日上限50条，不会漏，但是浪费API）\n",
    "getVideosbyDay(\"Guardian\",\"2021-02-24\",\"2021-04-30\")\n",
    "# 每天只能进行100次search，API配额用完了会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8e2a5629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并csv文件到大表\n",
    "combineCsv(\"BBC\",\"Day\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:socialEnv] *",
   "language": "python",
   "name": "conda-env-socialEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
